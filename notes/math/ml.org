#+title: Machine Learning Notes
#+author: Ben Sima

Sporadic notes from my studies in machine learning. A few general sources:

- [[https://www.coursera.org/learn/machine-learning/][Andrew Ng's Coursera Class]]

* Quick Overview and Definitions

- Machine Learning :: "giving computers the ability to learn without being
     explicitly programmed" ---Arthur Samuel
- Supervised learning :: an algorithm is given "right answers" as part of the
     dataset
- Unsupervised learning :: the algorithm does not have example "right answers,"
     also known as "clustering."
- Regression problem :: we are trying to predict a continuous valued output
- Classification problem :: we are trying to predict a discrete valued output
- Vectorization :: math trick that lets us
- Training Set :: the example data ("right answers", as above) given to a
     learning algorithm in order to discover potentially predictive
     relationships.
- Learning Algorithm :: Instructions, encoded into a function \(h\) and informed
     by the trainging set, which tell the computer how to understand and predict
     new data.

Conventially:

- \(h\) :: by convention, stands for hypothesis. A function the maps input /x/'s
     to output /y/'s.
- \(g\) :: No idea... some function I guess
- \(x\) :: sample values
- \(y\) :: predicted values
- \(m\) :: number of training examples
- \(\theta\) :: parameters of the model

* Supervised Learning

The term "supervised" refers to the fact that we supplied the algorithm with
"right answers." That is, for every example in a dataset, we also give the
correct algorithm output.

This is also called a /regression problem/, which means that we are trying to
prediction a continuous valued output (as opposed to a discrete value).

The opposite of regression is /classification/, wherein there is a discrete
valued output. In particular, the output can be 0 or 1 for false or true
respectively.

* Unsupervised Learning

We don't tell the algorithm in advance which elements go into which cluster.
Able to find structure from seemingly unstructured information.

- Clustering algorithms

Example 1: is Google News, which clusters together multiple news sources about a
single story.

Example 2: Market segmentation. Taking arbitrary demographics information, we
can cluster people into segments where they have desired similarities.

** Cocktail party problem

Say we have a cocktail party and two people are speaking into two microphones.
Each microphone will pick up some of the opposite person. An unsupervised
learning algorithm can separate the two audio streams, thus focusing on only the
dominant audio input.

The cocktail party algorithm can be done in one line of Octave code:

#+BEGIN_SRC octave
[W,s,v] = svd((repmat(sum(x .* x, 1), size(x, 1), 1) .* x) * x');
#+END_SRC

~svd~ is a linear algebra routine built into Octave.

* Cost Function

\( J(\theta_0, \theta_1) = \frac{1}{2m} \sum{m}{i=1} (h_\theta (x^{(1)}) - y^{(1)})^3 \)

The goal is to minimize for \( \theta_0, \theta_1\) \( J(\theta_0, \theta_1) \).
That is, you want to find the minimum value of the \( J \) function.

Basically, the \(J\) function measures the distance between every \(x\) and \(y\),

* Algorithms
** Gradient Descent

Gradient descent is kinda like a general framework for creating models...

Actually a general solution to finding minimum of J(theta_0 .. theta_n)

Outline

1. Start with some \( \theta_0, \theta_1 \).
2. Keep changing \( \theta_0, \theta_1 \) to reduce \( J( \theta_0, \theta_1 )
   \) until we hopefully end up at a minimum.

Visually, looks like walking down a hill.

#+BEGIN_LaTeX
\text{repeat until convergence} \{
  \theta_j := \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta_0, \theta_1) \text{(for j = 0 and j = 1)}
\}

\text{Simultaneiously update j=0 and j=1:}

\text{temp0} := \theta_0 - \\alpha \frac{\delta}{\delta \theta_j} J(\theta_0, \theta_1)
\text{temp1} := \theta_0 - \\alpha \frac{\delta}{\delta \theta_j} J(\theta_0, \theta_1)
\theta_0 := \text{temp0}
\theta_1 := \text{temp1}
#+END_LaTeX

Note that \(\alpha\) is the *learning rate*.

** Other algorithms

These algorithms have no need to manually pick \(\alpha\) and are often faster
than gradient descent. On the other hand, they are more complex.

- Conjugate gradient
- BFGS
- L-BFGS

* TODO Vectorization

FIXME

* TODO Model Representation
* Papers

- [[http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf][Hidden technical debt in ML systems]] - pretty good, should revisit once I write
  larger systems

** To Read

[[http://www.sciencemag.org/content/350/6266/1332.full][Human-level concept learning through probabilistic program induction]].
